{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset: images of handwritten numbers\n",
    "\n",
    "#importing the correct packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io \n",
    "abs_path = '/Users/assel2/Documents/ML_coursera/machine-learning-ex3/machine-learning-ex3/ex3/ex3data1.mat'\n",
    "raw_data = scipy.io.loadmat(abs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 5000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = raw_data['X'] #accessing a dictionary \n",
    "Y = raw_data['y'].T\n",
    "print(type(Y), Y.shape)\n",
    "\n",
    "index = 100\n",
    "demo_image = X[index]\n",
    "\n",
    "demo_image = demo_image.reshape(20,20)\n",
    "imgplot = plt.imshow(demo_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing zeros in y vector with 10\n",
    "np.place(Y,Y==10,0)\n",
    "train_set_x = X\n",
    "train_set_y = Y\n",
    "m_train = X.shape[0]\n",
    "num_Features = X.shape[1]\n",
    "num_Labls = 10\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(1, 5000)\n",
      "Number of training examples: X = (3000, 400)\n",
      "Number of training examples: Y = (1, 3000)\n",
      "Number of validating examples: X = (1000, 400)\n",
      "Number of validating examples: Y = (1, 1000)\n",
      "Number of test examples: X = (1000, 400)\n",
      "Number of test examples: Y = (1, 1000)\n",
      "[10]\n"
     ]
    }
   ],
   "source": [
    "#shuffle the training data set, take 20% for validation, 20% for test and 60% for training\n",
    "#adjust the hyperparameters with the validation set\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "rand_shuffle = np.random.permutation(5000)\n",
    "np.take(X,rand_shuffle,axis=0,out=X)\n",
    "np.take(Y,rand_shuffle,axis=1,out=Y)\n",
    "X_train = X[0:3000,:]\n",
    "Y_train = Y[:,0:3000]\n",
    "X_valid = X[3000:4000,:]\n",
    "Y_valid = Y[:,3000:4000]\n",
    "X_test = X[4000:5000,:]\n",
    "Y_test = Y[:,4000:5000]\n",
    "print (\"Number of training examples: X = \" + str(X_train.shape))\n",
    "print (\"Number of training examples: Y = \" + str(Y_train.shape))\n",
    "print (\"Number of validating examples: X = \" + str(X_valid.shape))\n",
    "print (\"Number of validating examples: Y = \" + str(Y_valid.shape))\n",
    "print (\"Number of test examples: X = \" + str(X_test.shape))\n",
    "print (\"Number of test examples: Y = \" + str(Y_test.shape))\n",
    "\n",
    "print(Y_train[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "demo_image = X_train[index]\n",
    "\n",
    "demo_image = demo_image.reshape(20,20)\n",
    "imgplot = plt.imshow(demo_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is standardising the data set\n",
    "train_set_x = X/255\n",
    "print(train_set_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (7,10) and (7,5000) not aligned: 10 (dim 1) != 7 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f7815457820b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#since there are 10 classes then the output layer is 10 nodess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (7,10) and (7,5000) not aligned: 10 (dim 1) != 7 (dim 0)"
     ]
    }
   ],
   "source": [
    "#step by step check logic for matrix\n",
    "input_layer_n = np.random.randn(400,5000) #this is a matrix with 400 features and 5000 training examples\n",
    "W1 = np.random.randn(7,400) #these are my waights for the first hidden layer. there are 7 neurons (rows) and 400 columns since it is L-1 or number of features in X\n",
    "b1 = np.zeros([7,1])\n",
    "Z1 = np.dot(W1,input_layer_n)+b1\n",
    "A1 = np.tanh(Z1)\n",
    "\n",
    "W2 = np.random.randn(10,7) #since there are 10 classes then the output layer is 10 nodess\n",
    "b2 = np.zeros([10,1])\n",
    "Z2 = np.dot(W2.T,A1)+ b2\n",
    "A2 = sigmoid(Z2)\n",
    "print(A2)\n",
    "#this is what I want to do and what kind of dimensions I am looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    n_x_in = X.shape[1] # size of input layer\n",
    "    n_hid = 7\n",
    "    n_y_out = 10 # size of output layer\n",
    "    return (n_x_in, n_hid, n_y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 400\n",
      "The size of the hidden layer is: n_h = 7\n",
      "The size of the output layer is: n_y = 10\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X_train,Y_train)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros([n_h,1])\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros([n_y,1])\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.00419294  0.00535638  0.00507218 ...  0.00806333  0.00769611\n",
      "  -0.0071764 ]\n",
      " [-0.00371894  0.00685595  0.02405774 ... -0.01148474 -0.02314751\n",
      "  -0.00813946]\n",
      " [-0.00705835 -0.02314023  0.00161697 ...  0.00345224 -0.00916835\n",
      "   0.00562171]\n",
      " ...\n",
      " [ 0.00826022  0.007941   -0.00338335 ... -0.00249701  0.00177274\n",
      "  -0.00164061]\n",
      " [ 0.00574501  0.01424859 -0.00426309 ...  0.00740493  0.00623319\n",
      "   0.0031466 ]\n",
      " [-0.00373321 -0.00106294  0.00583441 ...  0.00577612 -0.00124509\n",
      "   0.00054159]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00037491  0.0061951  -0.0074819   0.01077584  0.00532821 -0.00019878\n",
      "   0.0028315 ]\n",
      " [-0.02175431 -0.00988336 -0.00815306  0.0078343   0.00443631  0.00703503\n",
      "  -0.01051425]\n",
      " [ 0.00579883 -0.00806108 -0.00951759 -0.01607754 -0.00321216  0.00973861\n",
      "  -0.00704204]\n",
      " [ 0.00761965 -0.00716408 -0.00350593  0.0031444  -0.00098949 -0.0045349\n",
      "   0.00184064]\n",
      " [ 0.00291454  0.00111923 -0.00866729  0.00997096  0.00189248  0.00917063\n",
      "   0.01351324]\n",
      " [-0.00189667 -0.00153712  0.01424924 -0.00388879  0.01156568 -0.00124802\n",
      "  -0.01332077]\n",
      " [-0.01667803 -0.00601024  0.00669866  0.00155454  0.00131694 -0.01114178\n",
      "   0.00143302]\n",
      " [ 0.00153983 -0.01330129 -0.00427119  0.00594577 -0.00071436  0.00145792\n",
      "  -0.01467931]\n",
      " [-0.00663484  0.00075339  0.01078641 -0.00566502  0.00189452 -0.00677503\n",
      "   0.01212041]\n",
      " [ 0.00279552 -0.01472427 -0.00779006 -0.00288991  0.00414939  0.00302951\n",
      "  -0.00377378]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    #assert(A2.shape == (10, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out the forward propogation function \n",
    "A2, cache = forward_propagation(X_train,parameters)\n",
    "#print(A2)\n",
    "#Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    #you need to update this to make it compatable with softmax \n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    #logprobs = np.multiply(np.log(A2),Y)+(1-Y)*(np.log(1-A2))\n",
    "    #cost = -1/m*(np.sum(logprobs))\n",
    "    logprobs = np.multiply(np.log(A2),Y)\n",
    "    cost = -1*(np.sum(logprobs))\n",
    "#     import pdb;pdb.set_trace()\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382620.6950430216\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(A2,Y_train,parameters)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(3000, 10)\n",
      "(1, 3000)\n",
      "[[ 3 10  7 ...  4  8 10]]\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])\n",
    "Y_hot = np.zeros([X_train.shape[1],10])\n",
    "print(Y_hot.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_train)\n",
    "ind_Y_train = Y_train -1\n",
    "Y_hot[np.arange(X_train.shape[1]), ind_Y_train] = 1\n",
    "print(Y_hot)\n",
    "Y_hot = Y_hot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    " \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "  \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "  \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m*(np.dot(dZ2,A1.T))\n",
    "    db2 = 1/m*np.sum(dZ2,axis = 1, keepdims = True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),(1-np.power(A1,2)))\n",
    "    dW1 = 1/m*np.dot(dZ1,X.T)\n",
    "    db1 = 1/m*np.sum(dZ1,axis = 1, keepdims = True)\n",
    "\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW1': array([[ 0.00000000e+00,  0.00000000e+00, -5.90312249e-10, ...,\n",
      "         1.32018812e-07, -1.34111055e-08,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -2.95838985e-09, ...,\n",
      "         2.99191438e-08, -3.20472773e-09,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  1.47495591e-09, ...,\n",
      "         5.02814944e-08, -4.44393982e-09,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -7.05659041e-10, ...,\n",
      "        -1.26780902e-08,  1.34476900e-09,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  2.19583378e-10, ...,\n",
      "        -4.29398293e-08,  4.51953659e-09,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00, -1.18166702e-09, ...,\n",
      "         5.97095500e-08, -6.13805909e-09,  0.00000000e+00]]), 'db1': array([[ 2.09662422e-05],\n",
      "       [ 2.12922038e-05],\n",
      "       [-4.37487230e-05],\n",
      "       [ 2.80592424e-05],\n",
      "       [-1.83151251e-05],\n",
      "       [ 9.97071481e-05],\n",
      "       [ 2.24415659e-05]]), 'dW2': array([[-2.48240367e-03, -4.35821931e-03, -3.93022484e-03,\n",
      "         6.55522856e-03,  4.09938375e-03, -6.50288020e-04,\n",
      "         1.26079614e-03],\n",
      "       [-2.51538194e-04,  1.74307368e-03, -3.05762933e-03,\n",
      "        -1.97229238e-03,  2.39119126e-03,  3.12898273e-03,\n",
      "        -5.52794278e-04],\n",
      "       [ 2.35085358e-03,  1.21765762e-03,  2.55921663e-04,\n",
      "         3.44453677e-03,  2.04747614e-03,  1.64198985e-03,\n",
      "         2.77379361e-03],\n",
      "       [ 4.60258238e-05, -2.70826840e-04,  9.51442997e-04,\n",
      "        -1.57475516e-03, -5.65160051e-03, -1.14076156e-03,\n",
      "        -2.76688309e-03],\n",
      "       [ 6.79953759e-04, -7.59454059e-04, -1.05050351e-03,\n",
      "         1.74864841e-03, -1.19267067e-03, -4.57700513e-04,\n",
      "        -9.55366753e-04],\n",
      "       [ 7.78173073e-04,  3.65788552e-03,  1.30121793e-04,\n",
      "        -3.12406533e-03, -1.14383094e-03,  3.46391057e-04,\n",
      "        -3.94703698e-04],\n",
      "       [-4.05822947e-03, -5.99806042e-03,  3.03335538e-03,\n",
      "         2.62867889e-03, -4.46123436e-04, -2.80948081e-03,\n",
      "         2.56465914e-04],\n",
      "       [-8.39016405e-04,  2.02448954e-03,  2.36526882e-03,\n",
      "        -1.94408008e-03,  3.58307559e-03,  1.10860253e-03,\n",
      "         7.12371668e-04],\n",
      "       [-1.34819692e-03, -6.42288285e-04,  2.49874543e-03,\n",
      "        -2.20952932e-04, -2.33755467e-03, -1.09954578e-03,\n",
      "        -2.15981334e-03],\n",
      "       [ 5.12437842e-03,  3.38574255e-03, -1.19649840e-03,\n",
      "        -5.54094674e-03, -1.34934649e-03, -6.81894909e-05,\n",
      "         1.82613383e-03]]), 'db2': array([[-0.00332835],\n",
      "       [-0.00031407],\n",
      "       [ 0.00296751],\n",
      "       [ 0.00075797],\n",
      "       [ 0.00793761],\n",
      "       [ 0.0018833 ],\n",
      "       [ 0.0033759 ],\n",
      "       [-0.00219941],\n",
      "       [-0.00676532],\n",
      "       [-0.00431513]])}\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(parameters,cache,X_train,Y_hot)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "   \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    W1 = W1 - (learning_rate*(dW1))\n",
    "    b1 = b1 - (learning_rate*(db1))\n",
    "    W2 = W2 - (learning_rate*(dW2))\n",
    "    b2 = b2 - (learning_rate*(db2))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = update_parameters(parameters,grads,learning_rate =1)\n",
    "#print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trial = np.random.randn(1,5000)\n",
    "\n",
    "Y_trial = [0 if i <=0.5 else 1 for i in Y_trial]\n",
    "Y_trial = np.array(Y_trial)\n",
    "Y_trial = Y_trial.reshape(-1,1)\n",
    "print(Y_trial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations, print_cost=False):\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate = 1.2 )\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6907.861216\n",
      "Cost after iteration 100: 1203.013447\n",
      "Cost after iteration 200: 734.901766\n",
      "Cost after iteration 300: 592.065652\n",
      "Cost after iteration 400: 497.693881\n",
      "Cost after iteration 500: 477.779662\n",
      "Cost after iteration 600: 396.098793\n",
      "Cost after iteration 700: 350.312302\n",
      "Cost after iteration 800: 314.546414\n",
      "Cost after iteration 900: 303.396467\n",
      "Cost after iteration 1000: 277.146383\n",
      "Cost after iteration 1100: 259.949997\n",
      "Cost after iteration 1200: 245.304883\n",
      "Cost after iteration 1300: 232.591145\n",
      "Cost after iteration 1400: 244.034036\n",
      "Cost after iteration 1500: 224.790374\n",
      "Cost after iteration 1600: 210.777559\n",
      "Cost after iteration 1700: 201.484017\n",
      "Cost after iteration 1800: 193.739505\n",
      "Cost after iteration 1900: 186.917101\n",
      "Cost after iteration 2000: 180.780372\n",
      "Cost after iteration 2100: 175.485237\n",
      "Cost after iteration 2200: 170.802804\n",
      "Cost after iteration 2300: 166.590326\n",
      "Cost after iteration 2400: 162.752967\n",
      "Cost after iteration 2500: 159.216549\n",
      "Cost after iteration 2600: 155.911667\n",
      "Cost after iteration 2700: 152.703961\n",
      "Cost after iteration 2800: 149.001859\n",
      "Cost after iteration 2900: 145.832052\n",
      "Cost after iteration 3000: 143.051392\n",
      "Cost after iteration 3100: 140.420830\n",
      "Cost after iteration 3200: 137.594879\n",
      "Cost after iteration 3300: 134.676254\n",
      "Cost after iteration 3400: 132.154357\n",
      "Cost after iteration 3500: 129.730932\n",
      "Cost after iteration 3600: 126.899323\n",
      "Cost after iteration 3700: 123.597534\n",
      "Cost after iteration 3800: 121.206622\n",
      "Cost after iteration 3900: 118.935161\n",
      "Cost after iteration 4000: 116.685463\n",
      "Cost after iteration 4100: 114.509331\n",
      "Cost after iteration 4200: 112.423517\n",
      "Cost after iteration 4300: 110.422326\n",
      "Cost after iteration 4400: 108.548602\n",
      "Cost after iteration 4500: 106.786655\n",
      "Cost after iteration 4600: 105.106222\n",
      "Cost after iteration 4700: 103.496667\n",
      "Cost after iteration 4800: 101.963925\n",
      "Cost after iteration 4900: 100.510688\n",
      "Cost after iteration 5000: 99.131864\n",
      "Cost after iteration 5100: 97.820389\n",
      "Cost after iteration 5200: 96.569849\n",
      "Cost after iteration 5300: 95.374711\n",
      "Cost after iteration 5400: 94.230080\n",
      "Cost after iteration 5500: 93.131491\n",
      "Cost after iteration 5600: 92.074772\n",
      "Cost after iteration 5700: 91.055908\n",
      "Cost after iteration 5800: 90.070842\n",
      "Cost after iteration 5900: 89.115045\n",
      "Cost after iteration 6000: 88.182517\n",
      "Cost after iteration 6100: 87.263408\n",
      "Cost after iteration 6200: 86.340515\n",
      "Cost after iteration 6300: 85.402900\n",
      "Cost after iteration 6400: 84.487175\n",
      "Cost after iteration 6500: 83.621653\n",
      "Cost after iteration 6600: 82.800465\n",
      "Cost after iteration 6700: 82.015046\n",
      "Cost after iteration 6800: 81.259350\n",
      "Cost after iteration 6900: 80.529001\n",
      "Cost after iteration 7000: 79.820437\n",
      "Cost after iteration 7100: 79.130281\n",
      "Cost after iteration 7200: 78.454733\n",
      "Cost after iteration 7300: 77.788761\n",
      "Cost after iteration 7400: 77.124893\n",
      "Cost after iteration 7500: 76.453214\n",
      "Cost after iteration 7600: 75.770391\n",
      "Cost after iteration 7700: 75.092181\n",
      "Cost after iteration 7800: 74.433027\n",
      "Cost after iteration 7900: 73.793586\n",
      "Cost after iteration 8000: 73.171147\n",
      "Cost after iteration 8100: 72.563541\n",
      "Cost after iteration 8200: 71.969085\n",
      "Cost after iteration 8300: 71.386235\n",
      "Cost after iteration 8400: 70.813295\n",
      "Cost after iteration 8500: 70.248006\n",
      "Cost after iteration 8600: 69.686866\n",
      "Cost after iteration 8700: 69.123709\n",
      "Cost after iteration 8800: 68.545911\n",
      "Cost after iteration 8900: 67.920722\n",
      "Cost after iteration 9000: 67.120829\n",
      "Cost after iteration 9100: 65.558056\n",
      "Cost after iteration 9200: 64.089963\n",
      "Cost after iteration 9300: 63.116587\n",
      "Cost after iteration 9400: 62.339312\n",
      "Cost after iteration 9500: 61.668355\n",
      "Cost after iteration 9600: 61.065576\n",
      "Cost after iteration 9700: 60.510890\n",
      "Cost after iteration 9800: 59.992289\n",
      "Cost after iteration 9900: 59.501954\n",
      "W1 = [[ 3.25736042e-03  8.06334647e-03  9.93170928e-03 ... -1.06745327e-02\n",
      "   6.62106023e-03  3.42036729e-03]\n",
      " [-1.58115458e-03 -3.32664284e-03  2.81587658e-03 ... -7.96501495e-03\n",
      "   4.43716265e-03  5.24473508e-03]\n",
      " [ 1.46708695e-02  5.59156053e-03  1.01385038e-02 ...  2.35421360e-03\n",
      "   8.10456010e-03 -7.20459670e-03]\n",
      " ...\n",
      " [ 7.28747187e-03  9.13516029e-03 -5.26917641e-04 ...  8.54685983e-03\n",
      "  -1.09470690e-02 -1.40522402e-02]\n",
      " [ 8.93668806e-03  9.40674558e-05 -9.75932783e-03 ... -4.12766112e-03\n",
      "  -1.25664629e-03 -2.72290935e-04]\n",
      " [ 3.69059218e-03 -5.23054403e-04 -9.86851554e-03 ...  1.35224535e-03\n",
      "   1.58969374e-02 -5.96484267e-03]]\n",
      "b1 = [[ 1.47936196]\n",
      " [ 0.32893769]\n",
      " [ 0.94770899]\n",
      " [ 0.78325489]\n",
      " [-0.16615217]\n",
      " [ 1.68578918]\n",
      " [ 1.02912373]]\n",
      "W2 = [[-2.83877155e+00 -3.78623360e+00  2.55962117e+00  4.76546195e+00\n",
      "  -4.74336473e+00  9.56438933e-01  9.59782436e-01]\n",
      " [-4.08369961e+00 -8.31721427e-01 -2.33311074e+00  2.63288393e+00\n",
      "   4.82317365e+00  1.42944816e-01  1.44298028e+00]\n",
      " [-1.56936982e+00 -1.20285895e-01 -4.03261945e+00  1.68866331e+00\n",
      "  -3.12427459e+00  3.18951757e+00 -4.10297588e+00]\n",
      " [ 5.86609878e+00  4.98573793e+00  6.21691047e+00  8.67971882e-01\n",
      "  -1.43416394e+00 -3.16655449e+00 -2.34916007e+00]\n",
      " [ 5.27162336e-01  2.02666106e+00  4.16497576e+00 -1.91557656e+00\n",
      "  -3.78215088e+00  1.85559047e+00  7.07398764e+00]\n",
      " [ 4.21512927e+00  1.35213111e+00 -4.08351329e+00  4.03200962e+00\n",
      "   2.86986748e+00 -1.04220806e+00 -9.17973994e-01]\n",
      " [-1.78180839e+00 -3.65469229e+00  4.86230685e+00 -6.50898768e-01\n",
      "   2.22206201e+00  5.76880427e+00 -5.12041248e+00]\n",
      " [-5.56122214e+00  1.30362698e+00 -3.60400797e+00 -4.47953769e+00\n",
      "  -4.32533244e-01 -6.16245029e+00  2.85039135e+00]\n",
      " [ 5.31109854e+00 -4.21459213e+00 -1.40552719e+00 -3.88183161e+00\n",
      "  -1.09002376e+00 -2.70998265e+00 -4.43853880e+00]\n",
      " [-2.42584034e-03  2.95933108e+00 -2.36725380e+00 -3.05971139e+00\n",
      "   4.68733166e+00  1.18750584e+00  4.60258057e+00]]\n",
      "b2 = [[ 0.86143275]\n",
      " [ 4.20028186]\n",
      " [ 1.16376636]\n",
      " [-1.9618644 ]\n",
      " [ 0.73434957]\n",
      " [-0.36155265]\n",
      " [ 0.47836235]\n",
      " [-0.99114014]\n",
      " [-0.91633989]\n",
      " [-3.2072958 ]]\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train, Y_hot, 7, num_iterations=10000, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://utkuufuk.com/2018/06/03/one-vs-all-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    #assert(A2.shape == (10, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A2_test, cache_test = forward_propagation(X_test,parameters)\n",
    "\n",
    "Y_hot_test = np.zeros([X_test.shape[1],10])\n",
    "ind_Y_test = Y_test -1\n",
    "Y_hot_test[np.arange(X_test.shape[1]), ind_Y_test] = 1\n",
    "print(Y_hot_test)\n",
    "Y_hot_test = Y_hot_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9%\n",
      "Accuracy: 9%\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy: %d' % float((np.dot(Y_hot_test[1],A2_test[1].T) + np.dot(1-Y_hot_test[1],1-A2_test[1].T))/float(Y_hot_test.size)*100) + '%')\n",
    "print ('Accuracy: %d' % float((np.dot(Y_hot_test[2],A2_test[2].T) + np.dot(1-Y_hot_test[2],1-A2_test[2].T))/float(Y_hot_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-7574c072a03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_nonzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "all_nonzeros = np.nonzero(testing).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "       4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9,\n",
      "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9]), array([ 13,  18,  40,  42,  59,  64,  73,  84, 118, 133, 152, 157, 162,\n",
      "       164, 166, 170, 176, 194, 203, 205, 211, 216, 239, 252, 254, 266,\n",
      "       285, 290, 310, 333, 352, 360, 362, 367, 380, 403, 416, 419, 428,\n",
      "       442, 443, 450, 462, 463, 485, 493, 502, 503, 515, 517, 518, 528,\n",
      "       538, 544, 551, 555, 556, 558, 589, 605, 612, 650, 653, 657, 662,\n",
      "       672, 681, 695, 720, 758, 762, 766, 771, 777, 785, 799, 806, 808,\n",
      "       816, 819, 828, 831, 842, 852, 861, 863, 866, 868, 881, 895, 918,\n",
      "       954, 976, 991, 995,   6,  15,  19,  22,  25,  39,  68,  71,  72,\n",
      "        93,  96, 115, 121, 130, 142, 144, 153, 163, 173, 184, 189, 192,\n",
      "       206, 230, 233, 251, 264, 278, 288, 289, 296, 297, 304, 320, 325,\n",
      "       331, 334, 373, 383, 385, 388, 391, 396, 404, 406, 407, 418, 424,\n",
      "       441, 446, 448, 449, 460, 465, 467, 470, 476, 478, 481, 484, 488,\n",
      "       550, 563, 566, 567, 588, 596, 604, 613, 618, 631, 645, 656, 664,\n",
      "       674, 699, 704, 705, 711, 714, 743, 765, 772, 780, 798, 810, 814,\n",
      "       818, 825, 838, 840, 849, 859, 867, 871, 873, 874, 875, 909, 926,\n",
      "       960, 985, 986, 988, 993,   0,   8,  11,  32,  34,  36,  52,  85,\n",
      "        88,  91, 104, 110, 136, 139, 151, 154, 207, 224, 234, 261, 275,\n",
      "       276, 277, 284, 294, 295, 309, 323, 335, 354, 377, 379, 381, 408,\n",
      "       415, 445, 458, 466, 469, 475, 486, 491, 494, 500, 504, 546, 553,\n",
      "       565, 582, 594, 606, 607, 626, 630, 634, 635, 638, 639, 644, 648,\n",
      "       668, 670, 685, 691, 735, 736, 737, 740, 764, 801, 845, 850, 853,\n",
      "       857, 865, 872, 877, 889, 891, 892, 893, 896, 901, 913, 917, 919,\n",
      "       929, 945, 946, 953, 956, 965, 970, 971, 980, 992, 998,  14,  21,\n",
      "        23,  41,  47,  50,  51,  58,  69,  83,  92, 100, 109, 123, 140,\n",
      "       148, 149, 155, 178, 182, 193, 201, 212, 215, 218, 222, 232, 248,\n",
      "       249, 253, 257, 258, 269, 273, 274, 280, 281, 291, 299, 316, 328,\n",
      "       345, 351, 359, 374, 399, 414, 432, 439, 447, 473, 495, 499, 520,\n",
      "       545, 568, 571, 574, 577, 579, 584, 597, 609, 624, 643, 671, 675,\n",
      "       680, 697, 702, 713, 715, 719, 722, 726, 733, 742, 759, 763, 773,\n",
      "       805, 807, 812, 815, 841, 843, 846, 848, 864, 879, 880, 900, 940,\n",
      "       949, 958, 963, 967, 974, 987,   1,   4,  30,  35,  65,  80,  81,\n",
      "        89,  90,  95,  98, 103, 117, 120, 124, 125, 129, 146, 156, 159,\n",
      "       168, 169, 204, 210, 219, 220, 231, 237, 241, 245, 262, 279, 293,\n",
      "       303, 321, 324, 332, 357, 363, 387, 390, 394, 395, 400, 405, 409,\n",
      "       411, 436, 452, 453, 455, 464, 479, 508, 512, 516, 519, 524, 526,\n",
      "       530, 542, 557, 562, 570, 581, 595, 601, 602, 610, 619, 636, 663,\n",
      "       678, 684, 696, 698, 706, 707, 712, 718, 727, 731, 734, 739, 741,\n",
      "       747, 749, 754, 760, 774, 776, 781, 786, 791, 793, 803, 813, 832,\n",
      "       835, 836, 847, 862, 888, 894, 904, 916, 937, 939, 943, 950, 952,\n",
      "       955, 975, 978, 984,   5,   7,  10,  24,  26,  28,  37,  43,  46,\n",
      "        48,  56,  60,  76,  79,  99, 101, 105, 108, 111, 150, 161, 174,\n",
      "       175, 177, 179, 186, 187, 196, 221, 227, 228, 238, 246, 247, 250,\n",
      "       256, 302, 322, 329, 342, 375, 378, 454, 456, 471, 480, 489, 496,\n",
      "       497, 501, 513, 521, 525, 531, 552, 569, 585, 591, 593, 598, 615,\n",
      "       617, 620, 652, 661, 682, 683, 692, 716, 748, 753, 756, 770, 821,\n",
      "       822, 824, 829, 837, 851, 855, 860, 897, 924, 962, 964, 972, 973,\n",
      "       979, 997,   2,  17,  57,  63,  70,  78,  87, 116, 127, 131, 180,\n",
      "       181, 190, 198, 199, 200, 209, 225, 236, 260, 265, 268, 286, 287,\n",
      "       298, 301, 307, 311, 314, 326, 330, 336, 337, 346, 348, 370, 386,\n",
      "       389, 392, 397, 429, 438, 440, 451, 477, 487, 492, 511, 514, 522,\n",
      "       523, 527, 536, 539, 540, 548, 560, 572, 586, 590, 600, 603, 629,\n",
      "       633, 640, 641, 649, 654, 665, 667, 676, 709, 717, 725, 729, 738,\n",
      "       744, 746, 767, 768, 775, 782, 784, 787, 788, 792, 796, 830, 854,\n",
      "       870, 883, 884, 885, 886, 890, 899, 902, 906, 908, 910, 922, 930,\n",
      "       932, 935, 938, 951, 957, 966, 990, 994, 996,  29,  33,  54,  55,\n",
      "        61,  75,  94, 106, 107, 114, 141, 158, 160, 172, 185, 188, 191,\n",
      "       197, 202, 213, 214, 217, 226, 243, 255, 263, 267, 272, 282, 283,\n",
      "       300, 305, 306, 308, 313, 338, 341, 349, 393, 398, 401, 413, 423,\n",
      "       431, 433, 437, 444, 459, 482, 490, 505, 506, 507, 510, 533, 535,\n",
      "       541, 547, 549, 564, 575, 576, 580, 599, 614, 622, 623, 625, 627,\n",
      "       642, 673, 677, 679, 690, 693, 694, 700, 701, 732, 752, 755, 769,\n",
      "       789, 823, 839, 876, 882, 903, 912, 921, 925, 936, 944, 947, 961,\n",
      "       981, 982,   3,   9,  16,  31,  45,  49,  77,  82,  97, 102, 119,\n",
      "       122, 126, 134, 147, 171, 195, 208, 223, 229, 235, 242, 244, 270,\n",
      "       292, 317, 327, 353, 356, 358, 382, 384, 410, 412, 420, 421, 426,\n",
      "       427, 430, 434, 435, 457, 461, 468, 472, 483, 529, 532, 554, 573,\n",
      "       578, 587, 592, 611, 616, 637, 647, 651, 655, 658, 669, 686, 688,\n",
      "       689, 708, 721, 728, 730, 745, 779, 783, 790, 795, 797, 802, 809,\n",
      "       826, 869, 878, 887, 905, 907, 927, 928, 933, 934, 942, 959, 969,\n",
      "       983,  12,  20,  27,  38,  44,  53,  62,  66,  67,  74,  86, 112,\n",
      "       113, 128, 132, 135, 137, 138, 143, 145, 165, 167, 183, 240, 259,\n",
      "       271, 312, 315, 318, 319, 339, 340, 343, 344, 347, 350, 355, 361,\n",
      "       364, 365, 366, 368, 369, 371, 372, 376, 402, 417, 422, 425, 474,\n",
      "       498, 509, 534, 537, 543, 559, 561, 583, 608, 621, 628, 632, 646,\n",
      "       659, 660, 666, 687, 703, 710, 723, 724, 750, 751, 757, 761, 778,\n",
      "       794, 800, 804, 811, 817, 820, 827, 833, 834, 844, 856, 858, 898,\n",
      "       911, 914, 915, 920, 923, 931, 941, 948, 968, 977, 989, 999]))\n"
     ]
    }
   ],
   "source": [
    "print(all_nonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9]),\n",
       " array([ 13,  18,  40,  42,  59,  64,  73,  84, 118, 133, 152, 157, 162,\n",
       "        164, 166, 170, 176, 194, 203, 205, 211, 216, 239, 252, 254, 266,\n",
       "        285, 290, 310, 333, 352, 360, 362, 367, 380, 403, 416, 419, 428,\n",
       "        442, 443, 450, 462, 463, 485, 493, 502, 503, 515, 517, 518, 528,\n",
       "        538, 544, 551, 555, 556, 558, 589, 605, 612, 650, 653, 657, 662,\n",
       "        672, 681, 695, 720, 758, 762, 766, 771, 777, 785, 799, 806, 808,\n",
       "        816, 819, 828, 831, 842, 852, 861, 863, 866, 868, 881, 895, 918,\n",
       "        954, 976, 991, 995,   6,  15,  19,  22,  25,  39,  68,  71,  72,\n",
       "         93,  96, 115, 121, 130, 142, 144, 153, 163, 173, 184, 189, 192,\n",
       "        206, 230, 233, 251, 264, 278, 288, 289, 296, 297, 304, 320, 325,\n",
       "        331, 334, 373, 383, 385, 388, 391, 396, 404, 406, 407, 418, 424,\n",
       "        441, 446, 448, 449, 460, 465, 467, 470, 476, 478, 481, 484, 488,\n",
       "        550, 563, 566, 567, 588, 596, 604, 613, 618, 631, 645, 656, 664,\n",
       "        674, 699, 704, 705, 711, 714, 743, 765, 772, 780, 798, 810, 814,\n",
       "        818, 825, 838, 840, 849, 859, 867, 871, 873, 874, 875, 909, 926,\n",
       "        960, 985, 986, 988, 993,   0,   8,  11,  32,  34,  36,  52,  85,\n",
       "         88,  91, 104, 110, 136, 139, 151, 154, 207, 224, 234, 261, 275,\n",
       "        276, 277, 284, 294, 295, 309, 323, 335, 354, 377, 379, 381, 408,\n",
       "        415, 445, 458, 466, 469, 475, 486, 491, 494, 500, 504, 546, 553,\n",
       "        565, 582, 594, 606, 607, 626, 630, 634, 635, 638, 639, 644, 648,\n",
       "        668, 670, 685, 691, 735, 736, 737, 740, 764, 801, 845, 850, 853,\n",
       "        857, 865, 872, 877, 889, 891, 892, 893, 896, 901, 913, 917, 919,\n",
       "        929, 945, 946, 953, 956, 965, 970, 971, 980, 992, 998,  14,  21,\n",
       "         23,  41,  47,  50,  51,  58,  69,  83,  92, 100, 109, 123, 140,\n",
       "        148, 149, 155, 178, 182, 193, 201, 212, 215, 218, 222, 232, 248,\n",
       "        249, 253, 257, 258, 269, 273, 274, 280, 281, 291, 299, 316, 328,\n",
       "        345, 351, 359, 374, 399, 414, 432, 439, 447, 473, 495, 499, 520,\n",
       "        545, 568, 571, 574, 577, 579, 584, 597, 609, 624, 643, 671, 675,\n",
       "        680, 697, 702, 713, 715, 719, 722, 726, 733, 742, 759, 763, 773,\n",
       "        805, 807, 812, 815, 841, 843, 846, 848, 864, 879, 880, 900, 940,\n",
       "        949, 958, 963, 967, 974, 987,   1,   4,  30,  35,  65,  80,  81,\n",
       "         89,  90,  95,  98, 103, 117, 120, 124, 125, 129, 146, 156, 159,\n",
       "        168, 169, 204, 210, 219, 220, 231, 237, 241, 245, 262, 279, 293,\n",
       "        303, 321, 324, 332, 357, 363, 387, 390, 394, 395, 400, 405, 409,\n",
       "        411, 436, 452, 453, 455, 464, 479, 508, 512, 516, 519, 524, 526,\n",
       "        530, 542, 557, 562, 570, 581, 595, 601, 602, 610, 619, 636, 663,\n",
       "        678, 684, 696, 698, 706, 707, 712, 718, 727, 731, 734, 739, 741,\n",
       "        747, 749, 754, 760, 774, 776, 781, 786, 791, 793, 803, 813, 832,\n",
       "        835, 836, 847, 862, 888, 894, 904, 916, 937, 939, 943, 950, 952,\n",
       "        955, 975, 978, 984,   5,   7,  10,  24,  26,  28,  37,  43,  46,\n",
       "         48,  56,  60,  76,  79,  99, 101, 105, 108, 111, 150, 161, 174,\n",
       "        175, 177, 179, 186, 187, 196, 221, 227, 228, 238, 246, 247, 250,\n",
       "        256, 302, 322, 329, 342, 375, 378, 454, 456, 471, 480, 489, 496,\n",
       "        497, 501, 513, 521, 525, 531, 552, 569, 585, 591, 593, 598, 615,\n",
       "        617, 620, 652, 661, 682, 683, 692, 716, 748, 753, 756, 770, 821,\n",
       "        822, 824, 829, 837, 851, 855, 860, 897, 924, 962, 964, 972, 973,\n",
       "        979, 997,   2,  17,  57,  63,  70,  78,  87, 116, 127, 131, 180,\n",
       "        181, 190, 198, 199, 200, 209, 225, 236, 260, 265, 268, 286, 287,\n",
       "        298, 301, 307, 311, 314, 326, 330, 336, 337, 346, 348, 370, 386,\n",
       "        389, 392, 397, 429, 438, 440, 451, 477, 487, 492, 511, 514, 522,\n",
       "        523, 527, 536, 539, 540, 548, 560, 572, 586, 590, 600, 603, 629,\n",
       "        633, 640, 641, 649, 654, 665, 667, 676, 709, 717, 725, 729, 738,\n",
       "        744, 746, 767, 768, 775, 782, 784, 787, 788, 792, 796, 830, 854,\n",
       "        870, 883, 884, 885, 886, 890, 899, 902, 906, 908, 910, 922, 930,\n",
       "        932, 935, 938, 951, 957, 966, 990, 994, 996,  29,  33,  54,  55,\n",
       "         61,  75,  94, 106, 107, 114, 141, 158, 160, 172, 185, 188, 191,\n",
       "        197, 202, 213, 214, 217, 226, 243, 255, 263, 267, 272, 282, 283,\n",
       "        300, 305, 306, 308, 313, 338, 341, 349, 393, 398, 401, 413, 423,\n",
       "        431, 433, 437, 444, 459, 482, 490, 505, 506, 507, 510, 533, 535,\n",
       "        541, 547, 549, 564, 575, 576, 580, 599, 614, 622, 623, 625, 627,\n",
       "        642, 673, 677, 679, 690, 693, 694, 700, 701, 732, 752, 755, 769,\n",
       "        789, 823, 839, 876, 882, 903, 912, 921, 925, 936, 944, 947, 961,\n",
       "        981, 982,   3,   9,  16,  31,  45,  49,  77,  82,  97, 102, 119,\n",
       "        122, 126, 134, 147, 171, 195, 208, 223, 229, 235, 242, 244, 270,\n",
       "        292, 317, 327, 353, 356, 358, 382, 384, 410, 412, 420, 421, 426,\n",
       "        427, 430, 434, 435, 457, 461, 468, 472, 483, 529, 532, 554, 573,\n",
       "        578, 587, 592, 611, 616, 637, 647, 651, 655, 658, 669, 686, 688,\n",
       "        689, 708, 721, 728, 730, 745, 779, 783, 790, 795, 797, 802, 809,\n",
       "        826, 869, 878, 887, 905, 907, 927, 928, 933, 934, 942, 959, 969,\n",
       "        983,  12,  20,  27,  38,  44,  53,  62,  66,  67,  74,  86, 112,\n",
       "        113, 128, 132, 135, 137, 138, 143, 145, 165, 167, 183, 240, 259,\n",
       "        271, 312, 315, 318, 319, 339, 340, 343, 344, 347, 350, 355, 361,\n",
       "        364, 365, 366, 368, 369, 371, 372, 376, 402, 417, 422, 425, 474,\n",
       "        498, 509, 534, 537, 543, 559, 561, 583, 608, 621, 628, 632, 646,\n",
       "        659, 660, 666, 687, 703, 710, 723, 724, 750, 751, 757, 761, 778,\n",
       "        794, 800, 804, 811, 817, 820, 827, 833, 834, 844, 856, 858, 898,\n",
       "        911, 914, 915, 920, 923, 931, 941, 948, 968, 977, 989, 999]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Y = np.nonzero(Y_hot_test)\n",
    "testing_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_nonzeros[1]==testing_Y[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,  18,  40,  42,  59,  64,  73,  84, 118, 133, 152, 157, 162,\n",
       "       164, 166, 170, 176, 194, 203, 205, 211, 216, 239, 252, 254, 266,\n",
       "       285, 290, 310, 333, 352, 360, 362, 367, 380, 403, 416, 419, 428,\n",
       "       442, 443, 450, 462, 463, 485, 493, 502, 503, 515, 517, 518, 528,\n",
       "       538, 544, 551, 555, 556, 558, 589, 605, 612, 650, 653, 657, 662,\n",
       "       672, 681, 695, 720, 758, 762, 766, 771, 777, 785, 799, 806, 808,\n",
       "       816, 819, 828, 831, 842, 852, 861, 863, 866, 868, 881, 895, 918,\n",
       "       954, 976, 991, 995,   6,  15,  19,  22,  25,  39,  68,  71,  72,\n",
       "        93,  96, 115, 121, 130, 142, 144, 153, 163, 173, 184, 189, 192,\n",
       "       206, 230, 233, 251, 264, 278, 288, 289, 296, 297, 304, 320, 325,\n",
       "       331, 334, 373, 383, 385, 388, 391, 396, 404, 406, 407, 418, 424,\n",
       "       441, 446, 448, 449, 460, 465, 467, 470, 476, 478, 481, 484, 488,\n",
       "       550, 563, 566, 567, 588, 596, 604, 613, 618, 631, 645, 656, 664,\n",
       "       674, 699, 704, 705, 711, 714, 743, 765, 772, 780, 798, 810, 814,\n",
       "       818, 825, 838, 840, 849, 859, 867, 871, 873, 874, 875, 909, 926,\n",
       "       960, 985, 986, 988, 993,   0,   8,  11,  32,  34,  36,  52,  85,\n",
       "        88,  91, 104, 110, 136, 139, 151, 154, 207, 224, 234, 261, 275,\n",
       "       276, 277, 284, 294, 295, 309, 323, 335, 354, 377, 379, 381, 408,\n",
       "       415, 445, 458, 466, 469, 475, 486, 491, 494, 500, 504, 546, 553,\n",
       "       565, 582, 594, 606, 607, 626, 630, 634, 635, 638, 639, 644, 648,\n",
       "       668, 670, 685, 691, 735, 736, 737, 740, 764, 801, 845, 850, 853,\n",
       "       857, 865, 872, 877, 889, 891, 892, 893, 896, 901, 913, 917, 919,\n",
       "       929, 945, 946, 953, 956, 965, 970, 971, 980, 992, 998,  14,  21,\n",
       "        23,  41,  47,  50,  51,  58,  69,  83,  92, 100, 109, 123, 140,\n",
       "       148, 149, 155, 178, 182, 193, 201, 212, 215, 218, 222, 232, 248,\n",
       "       249, 253, 257, 258, 269, 273, 274, 280, 281, 291, 299, 316, 328,\n",
       "       345, 351, 359, 374, 399, 414, 432, 439, 447, 473, 495, 499, 520,\n",
       "       545, 568, 571, 574, 577, 579, 584, 597, 609, 624, 643, 671, 675,\n",
       "       680, 697, 702, 713, 715, 719, 722, 726, 733, 742, 759, 763, 773,\n",
       "       805, 807, 812, 815, 841, 843, 846, 848, 864, 879, 880, 900, 940,\n",
       "       949, 958, 963, 967, 974, 987,   1,   4,  30,  35,  65,  80,  81,\n",
       "        89,  90,  95,  98, 103, 117, 120, 124, 125, 129, 146, 156, 159,\n",
       "       168, 169, 204, 210, 219, 220, 231, 237, 241, 245, 262, 279, 293,\n",
       "       303, 321, 324, 332, 357, 363, 387, 390, 394, 395, 400, 405, 409,\n",
       "       411, 436, 452, 453, 455, 464, 479, 508, 512, 516, 519, 524, 526,\n",
       "       530, 542, 557, 562, 570, 581, 595, 601, 602, 610, 619, 636, 663,\n",
       "       678, 684, 696, 698, 706, 707, 712, 718, 727, 731, 734, 739, 741,\n",
       "       747, 749, 754, 760, 774, 776, 781, 786, 791, 793, 803, 813, 832,\n",
       "       835, 836, 847, 862, 888, 894, 904, 916, 937, 939, 943, 950, 952,\n",
       "       955, 975, 978, 984,   5,   7,  10,  24,  26,  28,  37,  43,  46,\n",
       "        48,  56,  60,  76,  79,  99, 101, 105, 108, 111, 150, 161, 174,\n",
       "       175, 177, 179, 186, 187, 196, 221, 227, 228, 238, 246, 247, 250,\n",
       "       256, 302, 322, 329, 342, 375, 378, 454, 456, 471, 480, 489, 496,\n",
       "       497, 501, 513, 521, 525, 531, 552, 569, 585, 591, 593, 598, 615,\n",
       "       617, 620, 652, 661, 682, 683, 692, 716, 748, 753, 756, 770, 821,\n",
       "       822, 824, 829, 837, 851, 855, 860, 897, 924, 962, 964, 972, 973,\n",
       "       979, 997,   2,  17,  57,  63,  70,  78,  87, 116, 127, 131, 180,\n",
       "       181, 190, 198, 199, 200, 209, 225, 236, 260, 265, 268, 286, 287,\n",
       "       298, 301, 307, 311, 314, 326, 330, 336, 337, 346, 348, 370, 386,\n",
       "       389, 392, 397, 429, 438, 440, 451, 477, 487, 492, 511, 514, 522,\n",
       "       523, 527, 536, 539, 540, 548, 560, 572, 586, 590, 600, 603, 629,\n",
       "       633, 640, 641, 649, 654, 665, 667, 676, 709, 717, 725, 729, 738,\n",
       "       744, 746, 767, 768, 775, 782, 784, 787, 788, 792, 796, 830, 854,\n",
       "       870, 883, 884, 885, 886, 890, 899, 902, 906, 908, 910, 922, 930,\n",
       "       932, 935, 938, 951, 957, 966, 990, 994, 996,  29,  33,  54,  55,\n",
       "        61,  75,  94, 106, 107, 114, 141, 158, 160, 172, 185, 188, 191,\n",
       "       197, 202, 213, 214, 217, 226, 243, 255, 263, 267, 272, 282, 283,\n",
       "       300, 305, 306, 308, 313, 338, 341, 349, 393, 398, 401, 413, 423,\n",
       "       431, 433, 437, 444, 459, 482, 490, 505, 506, 507, 510, 533, 535,\n",
       "       541, 547, 549, 564, 575, 576, 580, 599, 614, 622, 623, 625, 627,\n",
       "       642, 673, 677, 679, 690, 693, 694, 700, 701, 732, 752, 755, 769,\n",
       "       789, 823, 839, 876, 882, 903, 912, 921, 925, 936, 944, 947, 961,\n",
       "       981, 982,   3,   9,  16,  31,  45,  49,  77,  82,  97, 102, 119,\n",
       "       122, 126, 134, 147, 171, 195, 208, 223, 229, 235, 242, 244, 270,\n",
       "       292, 317, 327, 353, 356, 358, 382, 384, 410, 412, 420, 421, 426,\n",
       "       427, 430, 434, 435, 457, 461, 468, 472, 483, 529, 532, 554, 573,\n",
       "       578, 587, 592, 611, 616, 637, 647, 651, 655, 658, 669, 686, 688,\n",
       "       689, 708, 721, 728, 730, 745, 779, 783, 790, 795, 797, 802, 809,\n",
       "       826, 869, 878, 887, 905, 907, 927, 928, 933, 934, 942, 959, 969,\n",
       "       983,  12,  20,  27,  38,  44,  53,  62,  66,  67,  74,  86, 112,\n",
       "       113, 128, 132, 135, 137, 138, 143, 145, 165, 167, 183, 240, 259,\n",
       "       271, 312, 315, 318, 319, 339, 340, 343, 344, 347, 350, 355, 361,\n",
       "       364, 365, 366, 368, 369, 371, 372, 376, 402, 417, 422, 425, 474,\n",
       "       498, 509, 534, 537, 543, 559, 561, 583, 608, 621, 628, 632, 646,\n",
       "       659, 660, 666, 687, 703, 710, 723, 724, 750, 751, 757, 761, 778,\n",
       "       794, 800, 804, 811, 817, 820, 827, 833, 834, 844, 856, 858, 898,\n",
       "       911, 914, 915, 920, 923, 931, 941, 948, 968, 977, 989, 999])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
